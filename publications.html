---
layout: default
---

<h1>Publications</h1>

<div>
  <div class="keyword-toggle-container">
  <span class="label">Filter by keywords</span>
    <span class="btn btn-sm btn-default btn-keyword-all">
        All
    </span>
    <span class="btn btn-sm btn-default btn-keyword-none">
      None
    </span>
  </div>
  <div class="keyword-btn-container">
    {% for keyword in page.keywords %}
      <span class="chip active btn-keyword">{{ keyword }}</span>
    {% endfor %}
  </div>
</div>

{% for year_item in page.items %}
  <div class="pub-list columns">
    {% unless year_item.year == 9999 %}
      <div class="column col-12">
        <h3 class="pub-year">{{ year_item.year }}</h3>
      </div>
    {% endunless %}
    {% for item in year_item.items %}
      <div class="column pub-item col-12" id="{{ item.slug }}">
        <div class="card-publication columns">
          <div class="card-image column col-2 col-sm-12">
            <a href="#pub={{ item.slug }}">
              <img class="rounded pub-image" src="images/publications/{{ item.slug }}.png">
            </a>
          </div>
          <div class="column col-10 col-sm-12">
            <div class="card-header">
              <h4 class="card-title">
                <a href="#pub={{ item.slug }}" class="pub-title">{{ item.title }}</a>
              </h4>
              <h6 class="card-subtitle">
                {{ item.authors }} ({{item.year}}){% if item.cite_text %}. {{ item.cite_text }}{% endif %}.
              </h6>
            </div>
            <div class="card-footer">
              {% if item.keywords.size > 0 %}
                <div class="pub-keywords">
                  {% for keyword in item.keywords %}
                    <span class="label pub-keyword">{{ keyword }}</span>
                  {% endfor %}
                </div>
              {% endif %}
              <div class="pub-btns">
                {% if item.pdf %}
                  <a class="btn btn-link btn-sm pub-btn tooltip"
                     data-tooltip="Get pdf"
                     href="{{ item.pdf }}">
                    <i class="fa fa-file-pdf-o"></i> pdf
                  </a>
                {% else %}
                  <a class="btn btn-link btn-sm pub-btn tooltip"
                     data-tooltip="Get pdf"
                     href="/pdfs/publications/{{ item.slug }}.pdf">
                    <i class="fa fa-file-pdf-o"></i> pdf
                  </a>
                {% endif %}
                {% if item.doi %}
                  <a class="btn btn-link btn-sm pub-btn tooltip"
                     data-tooltip="Open doi link"
                     href="https://doi.org/{{ item.doi }}"
                     target="_blank">
                    <i class="fa fa-external-link"></i> article
                  </a>
                  <a class="btn btn-link btn-sm pub-btn tooltip"
                     data-tooltip="Save to mendeley"
                     href="http://www.mendeley.com/import/?url=https://doi.org/{{ item.doi }}"
                     target="_blank">
                    <i class="fa fa-save"></i> save
                  </a>
                {% endif %}
                {% if item.preprint %}
                  <a class="btn btn-link btn-sm pub-btn tooltip"
                     data-tooltip="Open preprint"
                     href="{{ item.preprint }}"
                     target="_blank">
                    <i class="fa fa-external-link"></i> preprint
                  </a>
                {% endif %}
                {% if item.github %}
                  <a class="btn btn-link btn-sm pub-btn tooltip"
                     data-tooltip="Open github repository"
                     href="https://github.com/{{ item.github }}"
                     target="_blank">
                    <i class="fa fa-github-alt"></i> github
                  </a>
                {% endif %}
              </div>
              <div class="pub-abstract">
                <h6>Abstract</h6>
                  {{ item.abstract }}
              </div>
            </div>
          </div>
        </div>
      </div>
    {% endfor %}
  </div>
{% endfor %}

<div class="pub-list columns">
      <div class="column col-12">
        <h3 class="pub-year">2022</h3>
      </div>   
  
      <div class="column pub-item col-12" id="modeling-future">
        <div class="card-publication columns">
          <div class="card-image column col-2 col-sm-12">
            <a href="#pub=modeling-future">
              <img class="rounded pub-image" src="images/publications/backgroundnoise.jpg">
            </a>
          </div>
          <div class="column col-10 col-sm-12">
            <div class="card-header">
              <h4 class="card-title">
                <a href="#pub=modeling-future" class="pub-title">BlackFeather: A framework for Background Noise Forensics</a>
              </h4>
              <h6 class="card-subtitle">
                Qi Li, Giuliano Sovernigo and Xiaodong Lin, DFRWS USA 2022.
              </h6>
            </div>
            <div class="card-footer">
              
                <div class="pub-keywords">
                  
                    <span class="label pub-keyword">voice</span>
                  
                    <span class="label pub-keyword">forensics</span>
                  
                    <span class="label pub-keyword">speech</span>
                  
                </div>
              
              <div class="pub-btns">
                
                  <a class="btn btn-link btn-sm pub-btn tooltip" data-tooltip="Get pdf" href="https://dfrws.org/conferences/dfrws-usa-2022/">
                    <i class="fa fa-file-pdf-o"></i> pdf
                  </a>
                
              </div>
              <div class="pub-abstract">
                <h6>Abstract</h6>
                  Historically, criminal investigations hinging on recorded audio data required manual application of forensic techniques to extract relevant information. These methods usually focus mainly on voices and speaker identification, but rarely focus on the wealth of forensic information available in the background noises present in the recording. Our paper introduces methods of automatically extracting, separating, and classifying background noises, allowing for the difficult, time-consuming process of audio analysis to be handled by software. Once the audio has been classified and examined by our proposed tools, the results can be used by investigators and forensic experts to aid in traditional investigative methods. Using environment information as an example, we propose a fully automated environment inference process based on background noise. Detailed experimental results show that our framework is effective and fast. Our proposed framework intends to provide a neat, automated, and accurate analysis of the information present in background audio, and to provide a new source of forensic information for investigators to leverage. In contrast to existing similar work, our scheme not only realistically considers mixed human voice speech, but also considers the case of multiple background noise mixes. To the best of our knowledge, this is the first forensic work that considers background noise in a complex environment.
              </div>
            </div>
          </div>
        </div>
      </div>  
  
      <div class="column col-12">
        <h3 class="pub-year">2022</h3>
      </div>
    
    
      <div class="column pub-item col-12" id="modeling-future">
        <div class="card-publication columns">
          <div class="card-image column col-2 col-sm-12">
            <a href="#pub=modeling-future">
              <img class="rounded pub-image" src="images/publications/voxstructor.jpg">
            </a>
          </div>
          <div class="column col-10 col-sm-12">
            <div class="card-header">
              <h4 class="card-title">
                <a href="#pub=modeling-future" class="pub-title">Voxstructor: Voice Reconstruction from Voiceprint</a>
              </h4>
              <h6 class="card-subtitle">
                Panpan Lu, Qi Li, Hui Zhu, Giuliano Sovernigo and Xiaodong Lin, Information Security Conference (ISC) (2021).
              </h6>
            </div>
            <div class="card-footer">
              
                <div class="pub-keywords">
                  
                    <span class="label pub-keyword">voice</span>
                  
                    <span class="label pub-keyword">privacy</span>
                  
                    <span class="label pub-keyword">security</span>
                  
                </div>
              
              <div class="pub-btns">
                
                  <a class="btn btn-link btn-sm pub-btn tooltip" data-tooltip="Get pdf" href="https://github.com/voxstructor/voxstructor/blob/main/Voxstructor_ISC21_paper.pdf">
                    <i class="fa fa-file-pdf-o"></i> pdf
                  </a>
                
                  <a class="btn btn-link btn-sm pub-btn tooltip"
                     data-tooltip="Open github repository"
                     href="https://github.com/voxstructor/voxstructor"
                     target="_blank">
                    <i class="fa fa-github-alt"></i> github
                  </a>
                
                
                
              </div>
              <div class="pub-abstract">
                <h6>Abstract</h6>
                  With the rapid development of machine learning technologies, voiceprint has become widely used as a personal identifier in daily life. 
Because of that, it is essential to determine to what extent a voiceprint derived from machine learning can be inverted to obtain the original speaker characteristic. However, the reconstruction of voiceprint templates is still a challenging issue. It has also not been proven whether the widespread use of voiceprint poses a privacy leakage risk. 
In this paper, we implement the first comprehensive, holistic, and systematic reconstruction study targeting voiceprint templates. We present Voxstructor, a voiceprint-based voice constructor that can be used for bulk template reconstruction attacks. An attacker can reconstruct a new voice based only on the victim's voiceprint data instead of the voice itself. Specifically, we formalize the voice reconstruction work as an objective optimization problem and merge voice cloning with voiceprint template conversion work. 
We have conducted extensive experiments on multiple mapping models, loss functions, voiceprint template extraction models, scoring methods, and two types of speaker verification attacks. Thorough experiments show that our attacks are effective, achieving a fairly high success rate which is similar to the results generated by voice cloning methods. The time overhead of Voxstructor is far less than other attacks. 
Our study not only demonstrates the need for protection of voiceprint templates in speaker recognition systems, but also shows that Voxstructor can be used as a privacy measure tool for voiceprint privacy-preserving schemes.
              </div>
            </div>
          </div>
        </div>
      </div>
</div>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script type="text/javascript" src="{{ site.baseurl }}/assets/js/publications.js"></script>
